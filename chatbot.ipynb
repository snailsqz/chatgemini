{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb807ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d18b182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the lines from movie_lines.txt\n",
    "lines = {}\n",
    "with open('dataset/movie_lines.txt', 'r', encoding='iso-8859-1') as f:\n",
    "    for line in f:\n",
    "        # Split the line by the \" +++$+++ \" delimiter\n",
    "        parts = line.strip().split(' +++$+++ ')\n",
    "        if len(parts) == 5:\n",
    "            lines[parts[0]] = parts[4]\n",
    "\n",
    "#create the conversation pairs from movie_conversations.txt\n",
    "pairs = []\n",
    "with open('dataset/movie_conversations.txt', 'r', encoding='iso-8859-1') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' +++$+++ ')\n",
    "        if len(parts) == 4:\n",
    "            # The list of line IDs is the last part\n",
    "            conversation_ids = parts[3].strip()[1:-1].replace(\"'\", \"\").replace(\" \", \"\").split(',')\n",
    "            # Create pairs of consecutive lines in the conversation\n",
    "            for i in range(len(conversation_ids) - 1):\n",
    "                input_line = lines.get(conversation_ids[i])\n",
    "                target_line = lines.get(conversation_ids[i+1])\n",
    "                if input_line and target_line:\n",
    "                    pairs.append([input_line, target_line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad3833d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total conversation pairs: 221282\n",
      "Example pair: ['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\"]\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of conversation pairs\n",
    "print(f\"Total conversation pairs: {len(pairs)}\")\n",
    "# Print the first pair for verification\n",
    "print(f\"Example pair: {pairs[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "098b39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    s = s.lower().strip()\n",
    "\n",
    "    s = re.sub(r\"i'm\", \"i am\", s)\n",
    "    s = re.sub(r\"he's\", \"he is\", s)\n",
    "    s = re.sub(r\"she's\", \"she is\", s)\n",
    "    s = re.sub(r\"it's\", \"it is\", s)\n",
    "    s = re.sub(r\"they're\", \"they are\", s)\n",
    "    s = re.sub(r\"you're\", \"you are\", s)\n",
    "    s = re.sub(r\"what's\", \"what is\", s)\n",
    "    s = re.sub(r\"where's\", \"where is\", s)\n",
    "    # Remove all non-letter and non-number characters\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \"\", s)\n",
    "    # Remove extra whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Apply the normalization to all pairs\n",
    "cleaned_pairs = [[normalize_string(pair[0]), normalize_string(pair[1])] for pair in pairs if normalize_string(pair[0]) and normalize_string(pair[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e90f09a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cleaned conversation pairs: 221274\n",
      "Example cleaned pair: ['can we make this quick roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad again', 'well i thought wed start with pronunciation if thats okay with you']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total cleaned conversation pairs: {len(cleaned_pairs)}\")\n",
    "# Print the first cleaned pair for verification\n",
    "print(f\"Example cleaned pair: {cleaned_pairs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cbad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of all unique words to build the vocabulary\n",
    "all_words = set()\n",
    "for pair in cleaned_pairs:\n",
    "    for sentence in pair:\n",
    "        for word in sentence.split(' '):\n",
    "            all_words.add(word)\n",
    "\n",
    "# Create the word-to-ID and ID-to-word dictionaries\n",
    "word_to_id = {word: i for i, word in enumerate(list(all_words))}\n",
    "id_to_word = {i: word for word, i in word_to_id.items()}\n",
    "\n",
    "# Add special tokens for padding, start of sentence, and end of sentence\n",
    "word_to_id['<PAD>'] = len(word_to_id)\n",
    "word_to_id['<SOS>'] = len(word_to_id)\n",
    "word_to_id['<EOS>'] = len(word_to_id)\n",
    "id_to_word[len(id_to_word)] = '<PAD>'\n",
    "id_to_word[len(id_to_word)] = '<SOS>'\n",
    "id_to_word[len(id_to_word)] = '<EOS>'\n",
    "\n",
    "VOCAB_SIZE = len(word_to_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
